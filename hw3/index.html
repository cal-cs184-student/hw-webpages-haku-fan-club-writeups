<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Crystal Chen, Jessica He</div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-haku-fan-club-writeups/hw3/index.html">https://cal-cs184-student.github.io/hw-webpages-haku-fan-club-writeups/hw3/index.html</a>
		<br>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-haku-fan-club-3">https://github.com/cal-cs184-student/sp25-hw3-haku-fan-club-3</a>
		


		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		In this project, we implemented a pathtracer using topics like ray generation, primitive intersection, acceleration structures, global and indirect illumination, and adaptive sampling. 
		We were able to generate images with varying lighting and learned more about how light interacts with objects and the surrounding environment. 

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		We began with the ray generation step in the rendering pipeline, which generates camera rays pointing towards the scene, intersecting 
		the virtual camera sensor. To implement this, we first transformed the image coordinates to camera space by scaling by 2, translating by (-1, -1), adding the -1 z-value dimension, 
		and finally scaling it by \( (tan (0.5 &times hFov), tan (0.5 &times vFov), 1) \), resulting in new coordinates on a camera sensor centered at (0, 0, 1) with 
		bottom left corner at \( (-tan (0.5 &times hFov), -tan (0.5 &times vFov), -1) \) and upper right corner at \( (tan (0.5 &times hFov), tan (0.5 &times vFov), -1) \). A visual diagram of 
		this is shown below.

		Now, we know that the ray origin is (0, 0, 0), and the direction is the point on the camera sensor we just calculated (normalized). We transform both 
		of these values from camera space into world space and generate the ray with these values, as the final generated ray should be in world space. We also 
		set <code>min_t</code> and <code>max_t</code> to <code>nclip</code> and <code>fclip</code>, respectively, so that we only consider points within the clipping 
		plane. Conceptually, what ray generation is doing is determining the what objects appear and the amount of radiance at every point to render in the final image. 

		<br>
		<figure>
			<img src="p1-diagram.png" alt="Image space to camera space" style="width:60%"/>
			<figcaption>Image space to camera space</figcaption>
		</figure>
		<br>
		Then, we implemented with the primitive intersection step in the rendering pipeline. Given a ray and a primitive, such as a triangle or 
		a sphere, we could use the ray equation \(r(t) = o + td\), where \(o\) is the origin, \(t\) is the time, and \(d\) is the unit direction of 
		the ray. By solving for the intersection of a sphere with \((o + td - c)^2 - R^2 = 0\), we can find the time that the ray interesects the 
		shape and use this to find the interesection point (derivation and diagram below). If there is more than one valid intersecion, we choose the one that first occurs. In our implementation, 
		an intersection is only valid if its between <code>min_t</code> and <code>max_t</code> and updates <code>max_t</code> to be the nearest intersection 
		to ignore future intersections that are farther away. 
		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p1-diagram2.png" width="200px"/>
				  <figcaption>Ray intersecting sphere</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p1-intersection.png" width="400px"/>
				  <figcaption>intersection derivation</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		<br><br>
		
		A similar method can be used for ray-triangle intersection, but in our implementation, we used the Möller-Trumbore algorithm to avoid having to calculate 
		the plane equation and to skip to directly finding the time of intersection as well as the barycentric coordinates. The full algorithm and components are shown 
		below. We first calculated 2 edge vectors \(E_1\) and \(E_2\) pointing from \(P_0\) to the other 2 vertices. We then calculated a vector \(S\) pointing from \(P_0\) to the ray's origin. After this, we calculated vectors \(S_1\) and \(S_2\), which are the cross products of \(D\) and \(E_2\), and \(S\) and \(E_1\) respectively. 
		Finally, to get the vector \([t, b_1, b_2]\), we divide \([S_2⋅E_2, S_1 ⋅ S, S_2 ⋅ D]\) by \(S_1 ⋅ E_1\). 
		Even though the algorithm only gives us 2 barycentric coordinates, \(b_1\) and \(b_2\), we can use the property that barycentric coordinates sum to 1 to derive the 
		last coordinate. We then check the validity of the intersection using the barycentric coordinates (they should all be between 0 and 1 inclusive and sum up to 1) 
		as well as if the computed time is within the range of <code>min_t</code> and <code>max_t</code>, updating <code>max_t</code> if it is valid. Our final intersection 
		point is \((1 - b_1 - b_2)P_0 + b_1P_1 + b_2P_2\). 

		<br><br>
		<figure>
			<img src="p1-moller-trumbore.png" alt="Möller-Trumbore algorithm" style="width:50%"/>
			<figcaption>Möller-Trumbore algorithm</figcaption>
		</figure>
		<br>

		Images with normal shading for a few small .dae files after implementing ray generation & primitive intersection:
		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p1-spheres.png" width="300px"/>
				  <figcaption>CBspheres_lambertian.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p1-teapot.png" width="300px"/>
				  <figcaption>teapot.dae</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p1-coil.png" width="300px"/>
					<figcaption>CBcoil.dae</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		
		<h2>Part 2: Bounding Volume Hierarchy</h2>

		<p>Here is our BVH construction algorithm: </p>
		<p> 
			The starter code expands a starting bounding box, <code>bbox</code>, to fit all the primitives that are given as input. If this number is less than or equal to <code>max_leaf_size</code>, then all we need to do is create a leaf node. So we build a <code>BVHNode</pcode> with its bounding box set to <code>bbox</code>, set its pointers to left and right children to null pointers and return this node!
			Otherwise, this node is not a leaf node because we have too many primitives. We’ll still set this node’s bounding box to <code>bbox</code>. Then, we find which axis <code>bbox</code> takes up the most space in, in other words, the axis in which its extent is the greatest. Choosing the axis with the greatest extent makes sense because we expect the objects inside to be more spread out within, which generally leads to balanced branches, which is good for runtime.
			To actually split the <code>bbox</code> on this axis, we chose to use the midpoint heuristic. The midpoint heuristic is good because in practice, we expect roughly half of the primitives to lie on each side of the midpoint (if there is some approximately uniform distribution). 
			We use the midpoint to split the groups into a left and a right partition. The left partition consists of primitives with bounding box centroids less than the midpoint on the earlier found axis. Since the given list isn’t necessarily sorted by our chosen axis, to actually partition out the primitives, we simply iterate through the given list of primitives and modify the list in-place. We swap any primitive at our current partition index, and increment the index until we’ve gone through the entire list. The result is that all the primitives in the left partition are located in the original list, indexed from <code>[start, partition)</code>, while the right partition is located at <code>[partition, end)</code>.
			However, one fault of the median heuristic point is that it is entirely possible that all primitives lie on one side of the midpoint. And in this case, this means nothing has been partitioned! When we make further recursive calls, it’ll therefore be on the same list of primitives, with the same longest axis, and we end up stuck in an infinite loop.
			Thus our solution is to check for this condition, and if it’s true, force partition point to be the middle primitive (middle meaning index-wise). The downside of this heuristic is that it doesn’t take into account the positions of the primitives’ bounding boxes, but at least we are guaranteed to prevent the infinite loop from occuring!
		</p>

		<br><br>
		Images with normal shading for a few large .dae files made renderable via BVH acceleration:
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p2-maxplanck.png" width="300px"/>
				  <figcaption>maxplanck.dae</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p2-CBlucy.png" width="300px"/>
				  <figcaption>CBlucy.dae</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p2-dragon.png" width="300px"/>
					<figcaption>dragon.dae</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<br><br>
		<br><br>
		Comparing rendering times of scenes with moderately complex geometries with and without BVH acceleration:
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
					<img src="p2-cow.png" width="300px"/>
					<figcaption>cow.dae with BVH acceleration; ~0.1128 seconds of rendering time</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p2-cow_slow.png" width="300px"/>
				  <figcaption>cow.dae without BVH acceleration; ~8.4320 seconds of rendering time- </figcaption>
				</td>
			</tr>
					<tr>
				<td style="text-align: center;">
					<img src="p2-beast_slow.png" width="300px"/>
					<figcaption>beast.dae with BVH acceleration; ~0.0365 seconds of rendering time</figcaption>
				  </td>
				  <td style="text-align: center;">
					  <img src="p2-beast.png" width="300px"/>
					  <figcaption>beast.dae without BVH acceleration; ~172.4064 seconds of rendering time</figcaption>
				</td>
			</tr>
			<tr>
				<td style="text-align: center;">
				  <img src="p2-CBlucy.png" width="300px"/>
				  <figcaption>CBlucy.dae with BVH acceleration; ~0.1265 seconds of rendering time</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p2-CBlucy_slow.png" width="300px"/>
					<figcaption>CBlucy.dae without BVH acceleration; ~480.4424 seconds of rendering time</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p>
			As expected, with or without BVH acceleration, we get the same rendering of the images. However, there is a significant difference in performance. We can take an example of a relatively simpler scene, <code> cow.dae </code> with 5,856 primitives. With BVH acceleration, it took 0.1128 seconds to render, compared to 8.4320 seconds without. 
		</p>
		
		<p>
			Another example is <code> beast.dae </code>, which is a more complex scene with 64,618 primitives. Rendering time was 0.0365 seconds with BVH acceleration (even less than it did for <code> cow.dae </code>!), and 172.4064 seconds without. 
		</p>
		<p>
			A much larger scene was <code> CBlucy.dae </code> with 133.796 primtitives. Though it technically can be rendered with BVH acceleration, the time it takes is far longer, from 0.1265 seconds to render with BVH accleration, and 480.4 seconds without. In each of these examples, it took a very short amount of time to also do the work of building the BVH tree (respectively for the 3 <code> .dae </code> files, it was 0.0138, 0.0010 0.0378 seconds). Clearly from all of these examples, it can be seen how efficient BVH acceleration can be when rendering scenes with moderately complex, and very complex geometries. But in addition from just looking at time saved, we can also examine the boost in another important metric. For example, for <code> CBlucy.dae </code>, BVH acceleration cut the average number of intersection tests per ray from about 49,348 to 12.69, which is an incredible improvement.
		</p>

		<h2>Part 3: Direct Illumination</h2>

		In this part, we implemented direct lighting 2 different ways, through uniform hemispehere sampling and by importance sampling lights. 
		<ol>
			<li>Uniform hemisphere sampling: For this method, we randomly sampled directions in the hemisphere around the surface normal. First, we made a coordinate system for a hit point
				with the surface normal aligned with the Z direction. Then, we used the given <code>hemisphereSampler->get_sample()</code> method to generate <code>num_samples</code> directions and  
				transformed them to world space, storing them in the <code>w_in_world</code> vector. For each sampled direction, we generated a shadow ray from the hit point with the direction being the transformed sampled direction, <code>w_in_world</code>. For every ray, we set <code>min_t</code> to <code>EPS_F</code>, an epsilon constant 
				that can be used to avoid numerical precision issues and self-intersection. We then used <code>bvh->intersect</code> to check if the shadow ray intersects a light source. If so, we computed the emission by calling <code>ray_int.bsdf->get_emission()</code>, where 
				<code>ray_int</code> is where the ray intersects the light. We also evaluated the BSDF function <code>f(w_out, w_in_object)</code> for the incoming and outgoing directions. Finally, we used the reflectance equation, adding <code>(ray_emission * cos_theta * bsdf_sample * 2.0 * M_PI)/(num_samples)</code> to 
				our total sum of lighting for every sample. <code>cos_theta</code> is the dot product of the normal of the given intersection and <code>w_in_world</code>. We multiply by \(2π\) because the pdf of uniformly sampling 
				across a hemisphere is \((1 / 2π)\). </li>
				<br>
			<li>Importance sampling lights: For this method, instead of randomly sampling in the hemisphere, we focus on sampling from the light sources. For every scene light, we sampled once if its a point light and <code>ns_area_light</code> times otherwise. Then, for each sample, 
			we used <code>sample_L(hit_p, &w_in_world, &distToLight, &pdf)</code> to sample the light. The method returns the emitted radiance <code>L_i</code>, writes the sampled direction between <code>hit_p</code> and the light source to <code>wi</code>, the distance 
			between <code>hit_p</code> and the light source in the <code>wi</code> direction to <code>distToLight</code>, and the value of the pdf evaluated at the <code>wi</code> direction to <code>pdf</code>. We then transformed the sampled direction to object space, stored in <code>w_in_object</code>. After this, we 
			generated a shadow ray with <code>hit_p</code> and <code>w_in_world</code>, setting the <code>min_t</code> and <code>max_t</code> values to <code>EPS_F</code> and <code>distToLight - EPS_F</code> respectively to avoid intersecting with the light itself. Then, we checked if there aren't any other objects between the hit point and the light source by using 
			the <code>bvh->intersect</code> method. If this was the case, we evaluated the BSDF function <code>f(w_out, w_in_object)</code> and plugged this value into the sum of direct lighting, which is <code>(L_i * cos_theta * bsdf_sample)/(pdf * num_samples)</code>, where 
			<code>cos_theta</code> is the dot product of the normal of the given intersection and <code>w_in_world</code>. 
			</li>
		</ol>
		<br>

		Some images rendered with both implementations of the direct lighting function:
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p3-bunny.png" width="300px"/>
				  <figcaption><codea>CBbunny.dae</codea></figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p3-dragon.png" width="225px"/>
				  <figcaption><code>dragon.dae</code></figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p3-walle.png" width="300px"/>
					<figcaption><code>wall-e.dae</code></figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<br>
		Light sampling (1 sample per pixel) rendered with different amount of light rays:

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p3-bunny_1.png" width="200px"/>
				  <figcaption>1 light ray</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p3-bunny_4.png" width="200px"/>
				  <figcaption>4 light rays</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p3-bunny_16.png" width="200px"/>
					<figcaption>16 light rays</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p3-bunny_64.png" width="200px"/>
					<figcaption>64 light rays</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<br><br>

		With uniform hemisphere sampling, the sample directions are very random, with a lot of them not even hitting a light source and thus not contributing at all to total direct lighting. This means that many 
		of the samples are wasted, and there is much more noise and variance in the result compared to light sampling. Especially in scenes with very few light sources, the chance of a random sample direction hitting a light source is extremely low, meaning that uniform sampling is more inefficient, requiring a lot more samples to converge. For light sampling, since we focus specifically on the light sources, calculating total direct lighting 
		is more deliberate and much faster, which causes the image to render quicker and more accurately and reduces noise in the results. 

		<h2>Part 4: Global Illumination</h2>

		<p>Here is how we implemented our indirect lighting function:</p>
		<p>We implemented the function <code> at_least_one_bounce_radiance </code>, which recursively calls upon itself to simulate multiple light bounces. First we check if the depth of our given ray is at 0. If so, there can be no more calls and we just return 0, for there will be no illumination to be returned.  Else, we know there’s still at least a bounce to be made. Moving on, our next step was similar to the direct lighting functions, and we made a coordinate system for a hit point with the normal aligned with the Z direction. If it happened that the ray’s depth was 1, or <code> isAccumBounces = true </code>, we know that we should compute the direct lighting at this bounce, and we set the outgoing illumination, <code> L_out</code> to the output of <code> one_bounce_radiance </code>. This is because we are either at the very last bounce, thus we want the one-bounce radiance, or because we want this quantity since we’re accumulating radiance from all bounces anyways.
		</p>
		<p>
		If it happens that the ray’s depth is greater than 1, then we must keep recursing. Here is where Russian Roulette gets introduced. We continue recursing with a probability equal to <code> continuation_prob = 0.65 </code>, which we defined based on advice from the homework spec. However, if the current ray’s depth is also equal to the maximum ray depth, we know we must continue on with the function so the ray can take the first bounce no matter what the probabilistic outcome from earlier was. 
		We then sample the current amount of light based on our value of <code> w_in_world</code>, which we feed into our isect’s BSDF’s <code> sample_f </code> function. We generate a sample ray based on the world coordinates and hit point, making sure to set the new ray’s depth by the current ray’s depth - 1, as we’ve already used up a bounce. We now find ourselves with two different outcomes, but only if we weren’t stopped by Russian Roulette from earlier, and if our BVH actually intersects this sample_ray (since we need a valid intersection to continue).
	</p>
		<ol>
		<li>If we allow for accumulated bounces, or this sample ray has depth 0, then we’ll compute the sampled amount of light multiplied by <code> at_least_one_bounce_radiance</code> recursively called on <code> sample_ray</code>. We’ll have to “normalize” this quantity by multiplying by the dot product between the normal vector and <code> w_in_world</code> to account for light hitting the surface at an angle, divide by pdf, and multiply by the continuation_probability in order to keep this estimator unbiased. We then add this quantity to <code>L_out</code>, because this is the illumination from the consequent bounces that we want to accumulate together.</li>
		<li> Else, we don’t allow for accumulated bounces. In this case, instead of adding the above quantity to <code>L_out</code>, we directly set <code>L_out</code> to that quantity. This is because we want the radiance solely from the last bounce. </li>
		</ol>
		<p>
		Finally, we return whatever <code>L_out</code>, the outgoing illuminance is. This is how much light is coming from <code>at_least_one_bounce_radiance</code> from the given ray and intersection.

		</p>


		<br><br>
		Examples of images rendered with global (direct and indirect) illumination with 1024 samples per pixel:
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p4-global-blob.png" width="200px"/>
				  <figcaption> <code>blob.dae</code> rendered with global illumination</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p4-global-spheres.png" width="200px"/>
				  <figcaption><code>CBspheres_lambertian.dae</code> rendered with global illumination</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p4-direct-bunny.png" width="200px"/>
					<figcaption>  <code>CBbunny.dae</code> rendered with global illumination </figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<br><br>
		Now let's look at the  <code>CBbunny.dae</code> scene by comparing views with only direct vs indirect illumination, with 1024 samples per pixel for each image.
		Notice how one key difference is that in the indirectly illuminated image, the scene light is black. This is because the only light in this scene on any given surface must
		have first originated via another surface via bounces. Additionally, this image is much softer.

		The directly illuminated image has much more stronger shadows, because every part of the image that does not get directly hit by the scene light is black. Thus in contrast, we observe
		a black ceiling, as well as the bunny's underside and shadow being much more darker and contrasting.

		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p4-direct-bunny.png" width="200px"/>
				  <figcaption> <code>CBbunny.dae</code>with direct illumination only</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p4-indirect-bunny.png" width="200px"/>
				  <figcaption> <code>CBbunny.dae</code> with indirect illumination only</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		<br><br>

		Now, for  <code>CBbunny.dae</code>, let's render images for different values of m from 0-5. We use 1024 samples per pixel.
		First, let's examine the first row, where <code>isAccumBounces</code> is set to false. As m increases, our images get dimmer, as we examine latter bounces of light. The light source appears completely black (except when <code>m=0</code>).
		Comparing the cases when <code>m=2</code> to <code>m = 3</code>, we can see that <code>m=3</code> is much more dimmer. Notice that the lights that are on the wall the bunny is in front of, have the effect of to decreasing the overall shadows between the ceilings in the walls, and this result can be
		observed when <code>isAccumBounces</code> is true, and <code>m = 3</code>, where we observe softer shadows near the ceiling in contrast to lower m values. The overall effect of the individual 2nd and 3rd bounces when combined with previous bounces of light work together to create
		a much more realistic image, one that rasterization, which relies on optimizations and other poorer approximations, cannot produce, at least as realistically.
		
		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p4-global-m0-unaccum.png" width="200px"/>
				  <figcaption> <code>m = 0</code>, <code>isAccumBounces = false</code> </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p4-global-m1-unaccum.png" width="200px"/>
				  <figcaption> <code>m = 1</code>, <code>isAccumBounces = false</code> </figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p4-global-m2-unaccum.png" width="200px"/>
					<figcaption> <code>m = 2</code>, <code>isAccumBounces = false</code> </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-global-m3-unaccum.png" width="200px"/>
					<figcaption> <code>m = 3</code>, <code>isAccumBounces = false</code> </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-global-m4-unaccum.png" width="200px"/>
					<figcaption> <code>m = 4</code>, <code>isAccumBounces = false</code> </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-global-m5-unaccum.png" width="200px"/>
					<figcaption> <code>m = 5</code>, <code>isAccumBounces = false</code> </figcaption>
				  </td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="p4-global-m0-accum.png" width="200px"/>
				  <figcaption> <code>m = 0</code>, <code>isAccumBounces = true</code> </figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p4-global-m1-accum.png" width="200px"/>
				  <figcaption> <code>m = 1</code>, <code>isAccumBounces = true</code> </figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p4-global-m2-accum.png" width="200px"/>
					<figcaption> <code>m = 2</code>, <code>isAccumBounces = true</code> </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-global-m3-accum.png" width="200px"/>
					<figcaption> <code>m = 3</code>, <code>isAccumBounces = true</code> </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-global-m4-accum.png" width="200px"/>
					<figcaption> <code>m = 4</code>, <code>isAccumBounces = true</code> </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-global-m5-accum.png" width="200px"/>
					<figcaption> <code>m = 5</code>, <code>isAccumBounces = true</code> </figcaption>
				  </td>
			  </tr>
			</table>
		</div>
		<br><br>
		<br><br>

		Now, let's look at how our renders for  <code>CBbunny.dae</code> look when Russian Roulette (with a continuation probability of 0.65) is added. We try different values of m, and 1024 samples per pixel:
		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p4-m0-rr.png" width="200px"/>
				  <figcaption> <code>m = 0</code></figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p4-m1-rr.png" width="200px"/>
				  <figcaption> <code>m = 1</code></figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p4-m2-rr.png" width="200px"/>
					<figcaption> <code>m = 2</code> </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-m3-rr.png" width="200px"/>
					<figcaption> <code>m = 3</code> </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-m4-rr.png" width="200px"/>
					<figcaption> <code>m = 4</code></figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-m5-rr.png" width="200px"/>
					<figcaption> <code>m = 5</code> </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-m100-rr.png" width="200px"/>
					<figcaption> <code>m = 100</code> </figcaption>
				  </td>
			  </tr>	
			</table>
		</div>
		<br><br>
		<br><br>

		Finally, let's look at the <code>CBbunny.dae scene</code>, and compare how the render views differ when we use different sample-per-pixel rates and 4 light rays. As can be seen from the images below,
		as sample-per-pixel rate increases, the images become much less grainy, and the shadows of the whole image become smoother. Though we can always distinguish the bunny and the general scene, a larger sample-per-pixel rate
		yields smoother, less noisy/grainy and an overall more accurate rendering of our scene.

		<br><br>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p4-sample-1.png" width="200px"/>
				  <figcaption> <code>1 sample per pixel</code></figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p4-sample-2.png" width="200px"/>
				  <figcaption> <code>2 samples per pixel</code></figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p4-sample-4.png" width="200px"/>
					<figcaption> <code>4 samples per pixel</code> </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-sample-8.png" width="200px"/>
					<figcaption> <code>8 samples per pixel</code> </figcaption>
				  </td>
				  </tr>
				  <tr>
				  <td style="text-align: center;">
					<img src="p4-sample-16.png" width="200px"/>
					<figcaption> <code>16 samples per pixel</code></figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-sample-64.png" width="200px"/>
					<figcaption> <code>64 samples per pixel</code> </figcaption>
				  </td>
				  <td style="text-align: center;">
					<img src="p4-sample-1024.png" width="200px"/>
					<figcaption> <code>1024 samples per pixel</code> </figcaption>
				  </td>
			  </tr>	
			</table>
		</div>
		<br><br>

		<h2>Part 5: Adaptive Sampling</h2>
		With adaptive sampling, we dynamically determine the number of samples every pixel based on a statistical calculation of how many samples it takes to converge, which avoids unnecessarily using a high number of samples 
		for pixels that don't require many samples to converge and reduce noise. By focusing more effort on noisy areas that require more samples and avoiding unnecessary extra work on less difficult parts of the images, adaptive sampling helps 
		improve the overall performance of the rendering. 
		
		<br><br>
		
		To implement this, as we traced ray samples through a pixel using <code>generate_ray</code>, we decided if the pixel had converged by calculating the value \(I = 1.96 * σ / √n\), where \(n\) is the number of samples, and 
		\(σ\) is the standard deviation of the samples. In our code, we kept track of 2 variables <code>s1</code> and <code>s2</code>, where <code>s1</code> is the sum of all the samples' illuminance values,
		and <code>s2</code> is the sum of all the samples' illuminance^2 values. With these values, we can calculate \(σ^2 = 1/(n-1) * (s_2 - s_1^2/n)\) and \(μ = s_1/n\). Using these, we find \(I = 1.96 * σ / √n\), as mentioned earlier. The smaller the value of \(I\) is, the more confident that we can be that the sample has converged. Thus, we set a <code>maxTolerance</code> variable to 0.05 for a 95% confidence interval and check if 
		\(I ≤ maxTolerance * μ\), where \(μ\) is the mean of the samples. If so, we can be 95% confident that the sample has converged and terminate the sampling for that pixel. Otherwise, we continue sampling. Also, to avoid calculating 
		the statistical values too frequently, we set a variable <code>samplesPerBatch</code> (default 32 samples), which only checks if a pixel has converged every <code>samplesPerBatch</code> samples. 


		<br><br>
		Scene renderings with their sampling rates (2048 samples per pixel, 1 sample per light, max ray depth of 5):

		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="p5-bunny.png" width="200px"/>
				  <figcaption>CBbunny.dae rendering</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="p5-bunny_rate.png" width="200px"/>
				  <figcaption>CBbunny.dae sampling rates</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p5-dragon.png" width="200px"/>
					<figcaption>dragon.dae rendering</figcaption>
				</td>
				<td style="text-align: center;">
					<img src="p5-dragon_rate.png" width="200px"/>
					<figcaption>dragon.dae sampling rates</figcaption>
				</td>
			  </tr>
			</table>
		</div>

	
		</div>
	</body>
</html>